# -*- coding: utf-8 -*-
"""llmops_vectordatabase_qdrant_01.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZagJtIhKcSNq38sMQBkzgb9eZR_MNz0d

# Semantic Search  With Qdrant Vector database

In this walkthrough we will see how to use Pinecone for semantic search. To begin we must install the required prerequisite libraries:
# 1. Data Prep
# 2. Create Collection --similar to table
# 3. Insert data into Collection created in step 2
# 4. Perform Semantic Search
# 5. Delete Collection if not required.

## 1. Data Prep
"""
#!pip install -qU langchain langchain_community langchain_openai qdrant_client

from langchain_community.document_loaders import TextLoader
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter

# Load the document, split it into chunks, embed each chunk and load it into the vector store.
raw_documents = TextLoader("sotu_address_obama.txt", encoding='utf-8').load()    #   Doc ai Models advanced
text_splitter = CharacterTextSplitter(chunk_size=600,separator='\n',chunk_overlap=100)
chunks = text_splitter.split_documents(raw_documents)

len(chunks)

chunks[6]

import os
#import getpass
#os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')
from dotenv import load_dotenv
load_dotenv()

os.environ['OPENAI_API_KEY']=os.getenv('OPENAI_API_KEY')
os.environ['GROQ_API_KEY']=os.getenv('GROQ_API_KEY')

embeddings = OpenAIEmbeddings(model='text-embedding-ada-002')

len(chunks)

chunks[0].page_content

values=embeddings.embed_query(chunks[0].page_content)
len(values)

for chunck in chunks[0:1]:
  print(chunck.page_content)
  values=embeddings.embed_query(chunck.page_content)
  print(values)

from importlib import metadata
docs=[]
docs_json={}
id=0
for chunk in chunks:
  id+=1
  values=embeddings.embed_query(chunk.page_content)
  docs_json={
      'id':str(id),
      'vector':values,
      'metadata':{
      'text':chunk.page_content,
      'source':chunk.metadata['source'],
      'author':'Venkat',
      'createdBY':'12-15-2025'
      }
  }
  docs.append(docs_json)

print(len(docs))

"""## 2. Creating a qdrant collection
https://e6c8d218-76ad-4103-b592-1af351b2dedc.us-east-1-0.aws.cloud.qdrant.io:6333/dashboard
"""

from qdrant_client import models, QdrantClient
from qdrant_client.http import models as rest_models

qdrant_clster_url='https://e6c8d218-76ad-4103-b592-1af351b2dedc.us-east-1-0.aws.cloud.qdrant.io:6333'

vdb_client=QdrantClient(url=qdrant_clster_url,api_key=os.getenv('QDRANT_API_KEY'))

collection_name = 'semantic-search-obama-text-test01'

vdb_client.create_collection(
    collection_name=collection_name,
    vectors_config=models.VectorParams(
        size=1536,
        distance=models.Distance.COSINE
    ),
)

vdb_client.get_collection(collection_name=collection_name)

"""# 3. Insert data into Collection
"""

docs_formatted = []
for i,doc in enumerate(docs):
    # Create a PointStruct object for each document
    docs_formatted.append(rest_models.PointStruct(
        id=i + 1,
        vector=doc['vector'],
        payload=doc['metadata']
    ))

# Upload points to Qdrant
vdb_client.upload_points(
    collection_name=collection_name,
    points=docs_formatted
)

print(f"Successfully inserted {len(docs)} records into {collection_name}")

"""## 4. Making Queries - Semantic Search

Now let's query.
"""

query = "can you summarize what was obama said about schools?"

# create the query vector
#xq = model.encode(query).tolist()
xq = embeddings.embed_query(query)

results=vdb_client.query_points(
    collection_name=collection_name,
    query=xq,
    limit=3
).points

for result in results:
  print(result)

context=''
sources=[]
for result in results:
    print(result)
    context=context+result.payload['text']
    if result.payload['source'] not in sources:
        sources.append([result.payload['source'],result.score])
print(sources)

context

prompt=''''
Answer the following question based on the context and format the output for user friendly
question:{question}
context:{context}
'''.format(question=query,context=context)
prompt

# Commented out IPython magic to ensure Python compatibility.
# %pip install -qU langchain_groq

from langchain_groq import ChatGroq

llm=ChatGroq(model='llama3-70b-8192',temperature=0.5)

response=llm.invoke(prompt)

print(response.content)

"""# 5. Delete Collection if not required"""

#vdb_client.delete_collection(collection_name=collection_name)